{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing VGG-16 on Cat vs Dog Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3×3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU’s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "REBUILD_DATA = False    #Flag that indicates whether data needs preprocessing or not.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Switching to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Switched to GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU available:\",torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"Switched to GPU\")\n",
    "else:\n",
    "    device = torch.cuda.device('cpu')\n",
    "    print(\"Working on CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T1 = torch.empty((450,1024,1024),dtype = torch.float64)\n",
    "T1 = T1.to(device)\n",
    "print(\"Allocated space:\",torch.cuda.memory_allocated(device)/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T2 = torch.empty((200,1024,1024),dtype = torch.float64)\n",
    "T2 = T2.to(device)\n",
    "print(\"Allocated space:\",torch.cuda.memory_allocated(device)/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T3 = torch.empty((50,1024,1024),dtype = torch.float64)\n",
    "T3 = T3.to(device)\n",
    "print(\"Allocated space:\",torch.cuda.memory_allocated(device)/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T4 = torch.empty((20,1024,1024),dtype = torch.float64)\n",
    "T4 = T4.to(device)\n",
    "print(\"Allocated space:\",torch.cuda.memory_allocated(device)/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 = torch.empty((5,1024,1024),dtype = torch.float64)\n",
    "T5 = T5.to(device)\n",
    "print(\"Allocated space:\",torch.cuda.memory_allocated(device)/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and preprocessing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Methode below takes a looooong time to load and process data. Maybe its because the vstack function has bad time complexity. So, as the trainind_data gets bigger and bigger, adding new data to it becomes even slower. We'll instead do the preprocessing in the Dataset object, which will make sure the data doesnt get processed all at once. This was, even bad time complexity wont effect so much. This functino is run __only once__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsVsCats():\n",
    "    X=0\n",
    "    Y=0\n",
    "    IMG_SIZE\n",
    "    def __init__(self, size):\n",
    "        self.IMG_SIZE = size    #Input size for VGG-16\n",
    "        self.CATS = os.path.join(os.getcwd(),'PetImages\\\\Cat')  \n",
    "        self.DOGS = os.path.join(os.getcwd(),'PetImages\\\\Dog')\n",
    "        self.LABELS = {self.CATS:0,self.DOGS:1}\n",
    "        self.training_data = []\n",
    "        self.catcount = 0\n",
    "        self.dogcount = 0\n",
    "    def __call__(self):\n",
    "        self.flag = True\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                try:\n",
    "                    path = os.path.join(label,f)\n",
    "                    img = cv2.imread(path)\n",
    "                    img = cv2.resize(img, (self.IMG_SIZE,self.IMG_SIZE))\n",
    "#                   img = np.transpose(img,(2,0,1))\n",
    "#                     self.Y = np.array([self.LABELS[label]])\n",
    "#                     self.X = np.vstack((self.X,img))\n",
    "#                     self.Y = np.block([self.Y,self.LABELS[label]])\n",
    "                    self.training_data.append([np.array(img),self.LABELS[label]])\n",
    "                    if label==self.CATS:\n",
    "                        self.catcount+=1\n",
    "                    elif label==self.DOGS: \n",
    "                        self.dogcount+=1\n",
    "                except Exception as e:\n",
    "                    print(\"Image \",f,\" failed to load!\")\n",
    "                    pass\n",
    "        training_data = (self.X,self.Y)\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\",self.training_data)\n",
    "        print(\"Cats: \",self.catcount)\n",
    "        print(\"Dogs: \",self.dogcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data should be preprocessed only once. On subsequent runs, we set the flag <code>REBUILD_DATA</code> to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVsCats(IMG_SIZE)\n",
    "    dogsvcats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets print an image to see what our data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cats = os.path.join(os.getcwd(),\"PetImages\\\\Cat\")\n",
    "path = os.path.join(os.getcwd(),cats,os.listdir(cats)[0])\n",
    "img = plt.imread(path)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "cv2.imshow(\"Cat\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well thats a cute doggo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object recieves numpy array which have images in Channels-last format. Pytorch likes her channels first. So we tranpose the array. Any other transform passed as argument is also performed. Then the image and label is returned as tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data, transform = None, test = False):\n",
    "        self.data = data\n",
    "        self.Size = round(self.data.shape[0]/10)\n",
    "        if test:\n",
    "            self.data = self.data[-self.Size:]\n",
    "        else:\n",
    "            self.data = self.data[:-self.Size]\n",
    "        self.len = self.data.shape[0]\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        X,Y = self.data[idx,0],self.data[idx,1]\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining custom Module for VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general architecture of VGG-16 is very simple. Instead of varying hyperparameters, it uses repititive blocks with same hyperparameters and relies on its depth for learning complex features. A side effect is that VGG-16 requires a very long time to train. Training locally on ImageNet would probably take a month. We will define our custom module block wise.\n",
    "<img src = \"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\" width = 800>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16,self).__init__()\n",
    "        #Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, )\n",
    "        #Block 2\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "        self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv4.weight, nonlinearity='relu')\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        #Block 3\n",
    "        self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv5.weight, nonlinearity='relu')\n",
    "        self.conv6 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv6.weight, nonlinearity='relu')\n",
    "        self.conv7 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv7.weight, nonlinearity='relu')\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        #Block 4\n",
    "        self.conv8 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv8.weight, nonlinearity='relu')\n",
    "        self.conv9 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv9.weight, nonlinearity='relu')\n",
    "        self.conv10 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv10.weight, nonlinearity='relu')\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        #Block 5\n",
    "        self.conv11 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv11.weight, nonlinearity='relu')\n",
    "        self.conv12 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv12.weight, nonlinearity='relu')\n",
    "        self.conv13 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        nn.init.kaiming_uniform_(self.conv13.weight, nonlinearity='relu')\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        #Block 6\n",
    "        self.fc1 = nn.Linear(in_features = 25088, out_features = 4096)\n",
    "        self.fc2 = nn.Linear(in_features = 4096, out_features = 4096)\n",
    "        self.fc2 = nn.Linear(in_features = 4096, out_features = 2)\n",
    "        self.output = nn.Softmax(dim = 1)\n",
    "    def forward(self,X):\n",
    "        #Block 1\n",
    "        X = self.conv1(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv2(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.pool1(X)\n",
    "        #Block 2\n",
    "        X = self.conv3(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv4(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.pool2(X)\n",
    "        #Block3\n",
    "        X = self.conv5(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv6(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv7(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.pool3(X)\n",
    "        #Block 4\n",
    "        X = self.conv8(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv9(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv10(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.pool4(X)\n",
    "        #Block 5\n",
    "        X = self.conv11(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv12(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.conv13(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.pool5(X)\n",
    "        #Block 6\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.fc1(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.fc2(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That a huge class. Phew!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,criterion, train_loader, test_loader, EPOCHS):\n",
    "    '''\n",
    "    Function for training the neural network.\n",
    "    '''\n",
    "    try:\n",
    "        assert TEST_SIZE!= None\n",
    "    except:\n",
    "        print(\"Test size is not known. I need it to calculate accuracy.\")\n",
    "    TRAIN_LOSS = []\n",
    "    ACCURACY = []    #Accuracy on test set\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        #torch.cuda.empty_cache()  #Clears cache to make more VRAM available\n",
    "        print(\"Epoch: \",epoch)\n",
    "        print(\"\\tTraining-\")\n",
    "        LOSS = 0        \n",
    "        for x,y in tqdm(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            LOSS+= loss.data\n",
    "        TRAIN_LOSS.append(LOSS)\n",
    "    model.eval()\n",
    "    print(\"\\tValidating\")\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            yhat = model(x)\n",
    "            _,label = torch.max(yhat.data,1)\n",
    "            correct += (label==y).sum().item()\n",
    "        accuracy = correct/TEST_SIZE*100\n",
    "        print(\"Test set accuracy for epoch \",epoch+1,\" = \", accuracy,\"%\")\n",
    "        ACCURACY.append(accuracy)\n",
    "    data = {\"Loss\":TRAIN_LOSS, \"Accuracy\":ACCURACY}\n",
    "    return model,optimizer, data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"training_data.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating custom Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = dataset(data, transform = tf)\n",
    "test_data = dataset(data, transform = tf, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model, optimizer and loss function objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters()) #Default learning rate is 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Passing a debug data mini-batch for 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST_SIZE = 256\n",
    "debug_data = dataset(data[:TEST_SIZE], transform = tf)\n",
    "debug_loader = DataLoader(debug_data)\n",
    "\n",
    "#Calling train_model() function\n",
    "Model, Optimizer, Data = train_model(model, optimizer, criterion,debug_loader, debug_loader,1)\n",
    "torch.save(Model.state_dict(),os.path.join(os.getcwd(),\"debug_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Passing whole data for 1 epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.ipc_collect()  #Checks if any unused tensors can be cleared to free up space\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "\tTraining-\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196d5914f18a4500b98de5de70b20913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22451.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  1\n",
      "\tTraining-\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18637b64182541dd9c85bbc4095dfbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22451.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  2\n",
      "\tTraining-\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f91647ddee54645a8fd5103044d1569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22451.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  3\n",
      "\tTraining-\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3f8bc9d6ee4e999fba6af72150a501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22451.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  4\n",
      "\tTraining-\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db5008ac7824f848b8d478a7bbe3dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22451.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEST_SIZE =  data.shape[0]-round(data.shape[0]/10)\n",
    "train_loader = DataLoader(train_data)\n",
    "test_loader = DataLoader(test_data)\n",
    "#T2.to(torch.device('cpu'))\n",
    "#Calling train_model function\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()  #Checks if any unused tensors can be cleared to free up space\n",
    "Model, Optimizer, Data = train_model(model, optimizer, criterion,train_loader, test_loader,10)\n",
    "torch.save(Model.state_dict(),os.path.join(os.getcwd(),\"trained_model_1epoch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_stats(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
